# Kubernetes module

![](https://img.shields.io/badge/kubernetes-v1.19.4-green.svg)
![](https://img.shields.io/badge/ubuntu-20.04-blue.svg)

This module will create a new kubernetes cluster inside your VPC.

You can find the parameters [here](params.md)

Last tested with:

        Terraform v0.13.4
        + provider registry.terraform.io/hashicorp/aws v3.11.0
        + provider registry.terraform.io/hashicorp/http v2.0.0
        + provider registry.terraform.io/hashicorp/template v2.2.0

Support:

    k8s     1.12.x      NO
    k8s     1.13.12     YES
    k8s     1.14.x      ?
    k8s     1.15.x      YES
    k8s     1.16.x      YES
    k8s     1.17.x      YES
    k8s     1.18.8      YES
    k8s     1.19.4      YES

## Usage

- [Utilities](../../examples/)

Remember to [generate a kubeadm token](https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/#cmd-token-generate):

    kubeadm token generate

Be careful to pass the right subnets in availability_zone!

### Choose the version

You can choose what version of k8s to install passing this variables:

    k8s_deb_package_version           = "1.19.4"
    kubeadm_install_version           = "stable-1.19"

## Debug

To get the kubelet logs:

    journalctl -u kubelet

## Connect to the cluster

You can follow the guide here:

- [Utilities](../../utilities/)

## AMI

The module accepts two parameters:

- ami_id_controller
- ami_id_worker

If nothing is passed, the latest ubuntu will be fetched.
Something else too can be fetched if needed passing different parameters.

Following best practices.

## Notes

The bootstrap file will create a cluster via kubeadm.

    MASTER_IP=`aws ec2 describe-instances --filters "Name=tag:k8s.io/role/master,Values=1" "Name=tag:KubernetesCluster,Values=$CLUSTER_ID" --region='us-east-1' | grep '\"PrivateIpAddress\"' | cut -d ':' -f2 | cut -d'"' -f 2 | uniq`

I use this line to find MY master (it works also if you have multiple clusters).

    $CLUSTER_ID

Must be unique per cluster.

## Gotchas

### skip-preflight-checks

While configuring a new node, I was receiving the following error:

    [preflight] Some fatal errors occurred:
    /var/lib/kubelet is not empty

As a temporary fix I added

    --skip-preflight-checks

So now the new join call is:

    kubeadm join \
    --skip-preflight-checks \
    --token=$CONTROLLER_JOIN_TOKEN $MASTER_IP

## kube-bench

- [https://github.com/aquasecurity/kube-bench](https://github.com/aquasecurity/kube-bench)

Running the CIS benchmarks on the controllers still returns 5 FAILURES:

[FAIL] 1.1.12 Ensure that the etcd data directory ownership is set to etcd:etcd (Automated)
[FAIL] 1.1.19 Ensure that the Kubernetes PKI directory and file ownership is set to root:root (Automated)
[FAIL] 1.2.6 Ensure that the --kubelet-certificate-authority argument is set as appropriate (Automated)
[FAIL] 1.2.16 Ensure that the admission control plugin PodSecurityPolicy is set (Automated)
[FAIL] 1.3.6 Ensure that the RotateKubeletServerCertificate argument is set to true (Automated)

About the following:

- 1.1.12: kubeadm doesn't manage an ETCD user on the machine, just root. So there is nothing to change there.
- 1.1.19: The directory IS owned by root. I really don't understand why it complains.

More [here](result.txt)

## TODO

- check kubelet extra args if it's not deprecated
- Use the loadbalancer to register to the masters
- Use random provider for better naming for some resources which name is generated by tf
- Add ability to use spot nodes
- Install the spot helper for spot nodes

- Use datasource instead of heredoc
- Change ebs partition
- Kill the node if the node cannot connect to the master ip
- Fix CA verification
- Make KCTL_USER parametric
- FIX the bash
- Change the providers to be injected instead of defining in the module
- FIX internal_network_cidr
- Add a random provider to generate something flexible to use in naming
- Add tags on resources with path to the module they are defined it
- Health check on the asg is done via ELB (check for using ALB)
- Export the information needed to create a target group outside the module
- Change from launch configuration to launch template
- Rename all workers reference to nodes
- Fix/reduce IAM roles power
- Access logs for lbs